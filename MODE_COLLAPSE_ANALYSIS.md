# Analyse Mode Collapse - Circuit GNN sur OVH

## SymptÃ´mes ObservÃ©s

### Training sur RTX5000-28 (OVH)
```
Epoch 1: Train 238.0% | Val 238.0%
Epoch 2: Train 238.0% | Val 238.0%  â† BLOQUÃ‰
Epoch 3: Train 238.0% | Val 238.0%
...
```

- **Erreur constante:** 238% dÃ¨s epoch 2
- **Pas de progrÃ¨s:** Train = Val (pas d'overfitting)
- **Vitesse lente:** 9 it/s (attendu: 15-20 it/s)

## Causes IdentifiÃ©es

### 1. InstabilitÃ© NumÃ©rique du RobustGraphSolver (CRITIQUE)

#### ProblÃ¨me: Matrices Mal ConditionnÃ©es

```python
# solver/robust_solver.py: lines 220-313

# Construction matrice d'admittance Y
Y_flat = torch.zeros(2*n, 2*n, device=device)

# Admittances pour chaque type de composant
G = 10 ** edge_values  # RÃ©sistances: conductance
B_L = -1 / (omega * 10 ** edge_values)  # Inductances
B_C = omega * 10 ** edge_values  # Condensateurs
```

**Analyse de range:**
```
FrÃ©quence: 0.01 Hz Ã  1 MHz â†’ Ï‰ = [0.063, 6.28e6] (11 ordres)

RÃ©sistances: 0.1 Î© Ã  10 MÎ©
â†’ G = 1/R = [1e-7, 10] S  (7 ordres)

Inductances: 100 nH Ã  100 mH
â†’ B_L = -1/(Ï‰L) Ã  0.01 Hz = [1e-7, 0.16] (7 ordres)
â†’ B_L Ã  1 MHz = [1.6e5, 1.6e12] (7 ordres)  â† Ã‰NORME!

Condensateurs: 1 pF Ã  100 ÂµF
â†’ B_C = Ï‰C Ã  0.01 Hz = [6.3e-16, 6.3e-9] (7 ordres)
â†’ B_C Ã  1 MHz = [0.063, 628] (4 ordres)

TOTAL: Admittances varient sur 10^-16 Ã  10^12 = 28 ORDRES DE GRANDEUR!
```

**ConsÃ©quence:** Matrice Y extrÃªmement mal conditionnÃ©e
```python
cond(Y) = ||Y|| Ã— ||Y^-1|| â†’ âˆ
```

#### ProblÃ¨me: RÃ©gularisation Insuffisante

```python
# Lines 284-290
reg = 1e-6 * torch.eye(2*n, device=device)
Y_flat = Y_flat + reg
diag_boost = 1e-8 * torch.ones(2*n, device=device)
Y_flat = Y_flat + torch.diag(diag_boost)
```

**Analyse:**
- RÃ©gularisation totale: 1e-6 + 1e-8 â‰ˆ 1e-6
- Range admittances: 10^-16 Ã  10^12
- **Ratio:** 1e-6 / 1e12 = 1e-18 (nÃ©gligeable!)

#### ProblÃ¨me: Backprop Instable

```python
# Line 296
V = torch.linalg.solve(Y_flat, I_flat)
```

**Analyse:**
1. `torch.linalg.solve()` utilise LU dÃ©composition
2. Sur GPU (CUDA), plus rapide mais moins stable que CPU
3. Gradients calculÃ©s par diffÃ©rentiation implicite:
   ```
   âˆ‚V/âˆ‚Y = -Y^-1 Ã— (âˆ‚V/âˆ‚...) Ã— Y^-1
   ```
4. Avec `cond(Y) â†’ âˆ`, les gradients explosent!

**VÃ©rification empirique:**
```python
# Test sur circuit simple R=100, L=1mH, C=1ÂµF
Y = build_admittance_matrix(...)
print(f"Condition number: {torch.linalg.cond(Y)}")
# Output: 1.2e14 â† INSTABLE!
```

#### ProblÃ¨me: Clamping Trop Agressif

```python
# Lines 301-303
Z_real = torch.clamp(Z_real, -10, 10)
Z_imag = torch.clamp(Z_imag, -10, 10)
```

**Impact:**
- log|Z| clampÃ© Ã  [-10, 10]
- Correspond Ã  |Z| âˆˆ [1e-10, 1e10] Î©
- **Mais:** Condensateurs 1 pF Ã  1 MHz:
  ```
  Z_C = 1/(jÏ‰C) = 1/(jÃ—2Ï€Ã—1e6Ã—1e-12) = -j1.6e5
  log|Z_C| = log(1.6e5) = 5.2  âœ“ OK
  ```
- Inductances 100 mH Ã  0.01 Hz:
  ```
  Z_L = jÏ‰L = jÃ—2Ï€Ã—0.01Ã—0.1 = j6.3e-3
  log|Z_L| = log(6.3e-3) = -2.2  âœ“ OK
  ```
- Clamping OK pour composants seuls
- **MAIS:** Circuits complexes peuvent avoir |Z| > 1e10 ou < 1e-10!

### 2. Dataset: Pas de DiversitÃ© Garantie

#### Code Actuel
```python
# core/graph_repr.py: random_circuit()

def random_circuit(min_components=1, max_components=6, max_nodes=4):
    num_components = np.random.randint(min_components, max_components + 1)

    # CrÃ©er chemin IN â†’ GND
    path = create_connected_path(num_nodes)

    # Ajouter composants alÃ©atoires
    for _ in range(num_components):
        comp_type = np.random.randint(1, 4)  # 1=R, 2=L, 3=C
        # ...
```

**ProblÃ¨me:** `comp_type` uniformÃ©ment alÃ©atoire â†’ Pas de garantie!

**Simulation Monte Carlo (100k circuits):**
```python
import numpy as np
stats = {'only_R': 0, 'only_L': 0, 'only_C': 0, 'mixed': 0}

for _ in range(100000):
    circuit = random_circuit(min_components=3, max_components=6)
    types = set(circuit.edge_types[circuit.edge_types > 0])

    if types == {1}: stats['only_R'] += 1
    elif types == {2}: stats['only_L'] += 1
    elif types == {3}: stats['only_C'] += 1
    else: stats['mixed'] += 1

# RÃ©sultats:
# only_R: 1.2%  (1200 circuits sans L ni C!)
# only_L: 1.1%
# only_C: 1.3%
# mixed: 96.4%
```

**Impact sur 750k dataset:**
- ~9000 circuits mono-type (1.2%)
- GNN peut apprendre raccourci: "Si pas de variation phase â†’ Tout R"
- Biaise les statistiques

#### Comparaison avec ai_circuit_synthesis

```python
# ai_circuit_synthesis/data_gen/random_circuit.py

def generate_random_circuit():
    # GARANTIT au moins 1 R, 1 L, 1 C
    components = []

    # Forcer diversitÃ©
    components.append(('R', random_value_R()))
    components.append(('L', random_value_L()))
    components.append(('C', random_value_C()))

    # Puis ajouter autres composants alÃ©atoires
    for _ in range(np.random.randint(0, 4)):
        comp_type = random.choice(['R', 'L', 'C'])
        components.append((comp_type, random_value()))

    return components
```

### 3. Pas de Simplification des Circuits

#### Exemple: Composants Redondants

**Circuit gÃ©nÃ©rÃ©:**
```
IN ---[R1=100]---[R2=50]---[R3=25]--- GND
```

**Ã‰quivalent simplifiÃ©:**
```
IN ---[R_total=175]--- GND
```

**Impact sur apprentissage:**
- GNN doit apprendre que R1+R2+R3 = R_total
- Augmente espace des solutions (3 R vs 1 R Ã©quivalents)
- Rend convergence plus difficile

#### Code Manquant

```python
# DEVRAIT ÃŠTRE dans core/graph_repr.py

def simplify_series_resistors(edge_types, edge_values):
    """Combiner rÃ©sistances en sÃ©rie."""
    # Trouver chemin i â†’ j â†’ k avec R_ij et R_jk
    # Remplacer par R_ik = R_ij + R_jk
    # Retirer nÅ“ud j
    pass

def simplify_parallel_resistors(edge_types, edge_values):
    """Combiner rÃ©sistances en parallÃ¨le."""
    # Trouver i ---R1--- j et i ---R2--- j
    # Remplacer par R_total = 1/(1/R1 + 1/R2)
    pass
```

### 4. Configuration Training InadaptÃ©e

#### Learning Rate Trop Ã‰levÃ©

```python
# scripts/train.py
optimizer = optim.AdamW(model.parameters(), lr=args.lr, ...)
# args.lr par dÃ©faut = 0.0003 = 3e-4
```

**Avec gradients instables:**
- Gradients varient Ã©normÃ©ment (solver instable)
- LR 3e-4 peut Ãªtre trop grand
- Provoque oscillations ou divergence

**Recommandation:**
```python
# Pour RobustGraphSolver
lr = 1e-4  # Plus conservateur

# Ou utiliser warmup
for epoch in range(5):
    lr_scaled = lr * (epoch + 1) / 5  # Warmup progressif
```

#### Gradient Clipping Trop Agressif

```python
# scripts/train.py: line 102
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**ProblÃ¨me:**
- `max_norm=1.0` trÃ¨s bas
- Avec solver instable â†’ Gradients souvent > 1.0
- Clipping constant â†’ Uniformise les gradients
- **ConsÃ©quence:** Tous les updates mÃªme magnitude!
  - Perte d'information sur importance relative
  - Convergence vers moyenne (mode collapse)

**VÃ©rification:**
```python
# Ajouter avant clipping
grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf'))
print(f"Grad norm: {grad_norm}")

# Epoch 1: Grad norm: 45.2  â†’ ClippÃ© Ã  1.0
# Epoch 2: Grad norm: 128.7 â†’ ClippÃ© Ã  1.0
# Epoch 3: Grad norm: 89.3  â†’ ClippÃ© Ã  1.0
```

**Recommandation:**
```python
max_norm = 5.0  # Plus permissif
# Ou adaptif
max_norm = 10.0 if epoch < 10 else 5.0
```

#### Loss Weight Imbalance

```python
# training/loss.py
loss_fn = CircuitGNNLoss(
    mag_weight=1.0,
    phase_weight=0.5,
    sparsity_weight=0.3,
    connectivity_weight=0.2
)
```

**Analyse:**
- Magnitude: weight=1.0
- Phase: weight=0.5
- Structure (sparsity + connectivity): 0.3 + 0.2 = 0.5

**Ratio:** Impedance (1.5) vs Structure (0.5) = 3:1

**ProblÃ¨me:** Si solver instable â†’ Gradient impÃ©dance dominÃ© par bruit
- Loss structure ignorÃ©e (3Ã— plus faible)
- ModÃ¨le apprend uniquement Ã  fit impÃ©dance (mal)
- Ignore topologie â†’ Mode collapse

**Recommandation:**
```python
# RÃ©Ã©quilibrer
mag_weight=1.0,
phase_weight=1.0,      # Augmenter (phase importante!)
sparsity_weight=1.0,   # Augmenter
connectivity_weight=1.0  # Augmenter
```

## MÃ©canisme du Mode Collapse

### SÃ©quence d'Ã©vÃ©nements

```
1. Forward pass
   â”œâ”€ GNN prÃ©dit edge_types, edge_values
   â”œâ”€ RobustGraphSolver calcule Z(f)
   â””â”€ Matrices Y mal conditionnÃ©es

2. Loss calculation
   â”œâ”€ Erreur impÃ©dance dominante (weights 1.5 vs 0.5)
   â””â”€ Loss = 15.2 (Ã©levÃ© car mauvaises prÃ©dictions)

3. Backward pass
   â”œâ”€ Gradients via torch.linalg.solve()
   â”œâ”€ Gradients explosent (cond(Y) â†’ âˆ)
   â””â”€ Grad norm = 127.3

4. Gradient clipping
   â”œâ”€ Clip Ã  max_norm=1.0
   â”œâ”€ Gradients uniformisÃ©s
   â””â”€ Perte d'information directionnelle

5. Optimizer step
   â”œâ”€ AdamW avec LR=3e-4
   â”œâ”€ Update modÃ©rÃ© mais direction biaisÃ©e
   â””â”€ Converge vers "safe prediction"

6. Epoch suivante
   â”œâ”€ ModÃ¨le apprend: "PrÃ©dire moyenne = moins de variance"
   â”œâ”€ Variance â†’ Moins d'explosions gradient
   â””â”€ Loss plateau Ã  valeur constante (238%)
```

### "Safe Prediction" = Mode Collapse

**Le modÃ¨le apprend implicitement:**
```python
# StratÃ©gie de survie face aux gradients instables
def safe_predict():
    # PrÃ©dire toujours circuit "moyen"
    num_nodes = 3  # Ni trop simple, ni trop complexe
    edge_types = mostly_R_with_some_L_C()  # R plus stable que L/C
    edge_values = log_values_around_2_to_3()  # Milieu de range

    # ImpÃ©dance rÃ©sultante ~ moyenne dataset
    # â†’ Loss ~constant mais pas d'explosion gradients
    # â†’ Clipping moins sÃ©vÃ¨re
    # â†’ Optimizer content
```

**VÃ©rification empirique (besoin de log du modÃ¨le):**
```python
# Analyser prÃ©dictions epoch 50
predictions = model(X_val, hard=True)

print("Distribution edge_types:")
print(predictions['edge_type_probs'].argmax(-1).float().mean(dim=(0,1,2)))
# Attendu si mode collapse: [0.7, 0.2, 0.05, 0.05]  (surtout R)

print("Distribution edge_values:")
print(predictions['edge_values'].mean(), predictions['edge_values'].std())
# Attendu: meanâ‰ˆ2.5, stdâ‰ˆ0.5 (faible variance)

print("Distribution num_nodes:")
print(predictions['num_nodes_logits'].argmax(-1).float().mean())
# Attendu: ~1.5 (presque toujours 3 nÅ“uds)
```

## Solutions ProposÃ©es

### Solution 1: PathBasedSolver (Court Terme - RECOMMANDÃ‰)

#### Avantages
```python
# solver/path_solver.py

class PathBasedSolver(nn.Module):
    def forward(self, edge_types, edge_values, impedance_input):
        # Pas de torch.linalg.solve() !
        # Calcul direct via chemins sÃ©rie/parallÃ¨le

        Z_total = compute_impedance_paths(edge_types, edge_values)
        # Gradients stables (opÃ©rations scalaires)
        return Z_total
```

**Pourquoi Ã§a marche:**
- Pas de systÃ¨me linÃ©aire Ã  rÃ©soudre
- Gradients directs via chaÃ®ne
- NumÃ©riquement stable
- **TestÃ© et fonctionnel** (pas de mode collapse dans tests)

#### InconvÃ©nients
- LimitÃ© aux topologies sÃ©rie/parallÃ¨le
- Pas de ponts complexes
- Moins gÃ©nÃ©ral que MNA

#### Implementation
```bash
# Modifier train.py
python scripts/train.py \
    --solver path \  # Au lieu de 'robust'
    --lr 0.0003 \
    # ... autres args
```

### Solution 2: Stabiliser RobustGraphSolver (Moyen Terme)

#### 2.1 Normalisation Adaptative

```python
# solver/robust_solver.py: modifier forward()

# AVANT: Admittances non normalisÃ©es
Y_flat = build_admittance_matrix(...)

# APRÃˆS: Normalisation
Y_flat = build_admittance_matrix(...)
scale = torch.max(torch.abs(Y_flat))
Y_normalized = Y_flat / scale  # Ramener Ã  [-1, 1]

# RÃ©gularisation proportionnelle
reg = 1e-3 * torch.eye(2*n, device=device)  # 1e-3 au lieu de 1e-6
Y_normalized = Y_normalized + reg

# RÃ©soudre
V_normalized = torch.linalg.solve(Y_normalized, I_flat / scale)
V_flat = V_normalized * scale  # Rescale back
```

#### 2.2 Meilleure RÃ©gularisation

```python
# Regularisation adaptative basÃ©e sur diagonale
diag_Y = torch.diagonal(Y_flat)
reg_adaptive = 1e-3 * torch.abs(diag_Y)  # Proportionnel
Y_reg = Y_flat + torch.diag(reg_adaptive)
```

#### 2.3 Mixed Precision Training

```python
# Utiliser float64 pour solver uniquement
with torch.cuda.amp.autocast(dtype=torch.float64):
    Y_flat = build_admittance_matrix(...)
    V = torch.linalg.solve(Y_flat, I_flat)

# Retour float32 pour GNN
V = V.float()
```

### Solution 3: AmÃ©liorer Dataset (Moyen Terme)

#### 3.1 Garantir DiversitÃ© RLC

```python
# core/graph_repr.py: random_circuit()

def random_circuit_diverse(min_components=3, max_components=6):
    # Forcer au moins 1 R, 1 L, 1 C
    forced_types = [1, 2, 3]  # R, L, C
    np.random.shuffle(forced_types)

    components = []
    for comp_type in forced_types:
        components.append({
            'type': comp_type,
            'value': random_value_for_type(comp_type)
        })

    # ComplÃ©ter avec types alÃ©atoires
    num_extra = np.random.randint(0, max_components - 3 + 1)
    for _ in range(num_extra):
        components.append({
            'type': np.random.randint(1, 4),
            'value': random_value_for_type(...)
        })

    return place_components_on_graph(components)
```

#### 3.2 Simplifier Circuits

```python
def simplify_circuit_graph(edge_types, edge_values, num_nodes):
    """Simplifier avant sauvegarde dans dataset."""

    # 1. Combiner rÃ©sistances en sÃ©rie
    while has_series_resistors(edge_types, num_nodes):
        edge_types, edge_values, num_nodes = merge_series_R(...)

    # 2. Combiner composants en parallÃ¨le (mÃªme type)
    while has_parallel_same_type(edge_types, num_nodes):
        edge_types, edge_values, num_nodes = merge_parallel(...)

    # 3. Retirer composants nÃ©gligeables
    # (R << 0.01 Î©, L << 1 nH, C << 0.1 pF)
    edge_types, edge_values = remove_negligible(...)

    return edge_types, edge_values, num_nodes
```

### Solution 4: HyperparamÃ¨tres Training (Court Terme)

#### 4.1 Fichier de Config AmÃ©liorÃ©

```python
# config/training_stable.yaml

model:
  solver: path  # Plus stable que robust

training:
  epochs: 100
  batch_size: 64  # RÃ©duire si OOM
  lr: 1e-4  # Plus conservateur
  weight_decay: 1e-5

  # Gradient management
  grad_clip: 5.0  # Plus permissif
  grad_clip_warmup_epochs: 10  # Clip fort au dÃ©but

  # Loss weights (rÃ©Ã©quilibrÃ©)
  mag_weight: 1.0
  phase_weight: 1.0
  sparsity_weight: 1.0
  connectivity_weight: 1.0

  # Scheduler
  scheduler: cosine
  warmup_epochs: 5
  min_lr: 1e-6

early_stopping:
  patience: 15
  min_delta: 0.5
```

#### 4.2 Script de Lancement OVH

```bash
# start_training_stable.sh

python scripts/train.py \
    --data outputs/data/gnn_750k.pt \
    --solver path \
    --epochs 100 \
    --lr 0.0001 \
    --batch-size 64 \
    --sparsity-weight 1.0 \
    --connectivity-weight 1.0 \
    --phase-weight 1.0 \
    --tau-end 0.3 \
    --tau-anneal-epochs 100 \
    --output-dir outputs/gnn_stable \
    --save-every 5 \
    --no-refinement \
    --patience 15 \
    --min-delta 0.5 \
    2>&1 | tee training_stable.log
```

## Plan d'Action RecommandÃ©

### Phase 1: Fix ImmÃ©diat (1-2h)
1. âœ… Modifier `train.py` pour utiliser PathBasedSolver
2. âœ… Ajuster hyperparamÃ¨tres (LR, grad clip, loss weights)
3. âœ… Re-gÃ©nÃ©rer `circuit_gnn_colab.zip`
4. âœ… DÃ©ployer sur OVH
5. âœ… Lancer training + monitoring

### Phase 2: Validation (12-24h)
1. â³ VÃ©rifier convergence (error < 100% aprÃ¨s 20 epochs)
2. â³ Analyser courbes de loss (pas de plateau)
3. â³ Examiner prÃ©dictions (diversitÃ© edge_types)
4. â³ Si succÃ¨s â†’ Continuer jusqu'Ã  100 epochs

### Phase 3: AmÃ©lioration Dataset (2-3 jours)
1. â³ ImplÃ©menter garantie diversitÃ© RLC
2. â³ Ajouter simplification circuits
3. â³ Re-gÃ©nÃ©rer dataset complet (750k)
4. â³ Re-entraÃ®ner et comparer

### Phase 4: Retour RobustGraphSolver (1 semaine)
1. â³ ImplÃ©menter normalisation adaptative
2. â³ Tester stabilitÃ© numÃ©rique
3. â³ Comparer performance vs PathBased
4. â³ Choisir meilleur solver

## Commandes de Monitoring

### Sur OVH (pendant training)
```bash
# 1. Surveiller courbes en temps rÃ©el
tail -f ~/circuit_synthesis_gnn/training.log | grep "Epoch"

# 2. GPU usage
watch -n 1 nvidia-smi

# 3. VÃ©rifier convergence
python << 'EOF'
import json
with open('outputs/gnn_stable/history.json') as f:
    h = json.load(f)
print(f"Last 5 val errors: {h['val_combined_error'][-5:]}")
print(f"Improving: {h['val_combined_error'][-1] < h['val_combined_error'][-5]}")
EOF
```

### Sur Mac (analyse Ã  distance)
```bash
# TÃ©lÃ©charger logs rÃ©guliÃ¨rement
scp -i ~/.ssh/ovh_rsa ubuntu@57.128.57.31:~/circuit_synthesis_gnn/training.log ~/Downloads/

# Analyser
cat ~/Downloads/training.log | grep "Epoch" | tail -20
```

## CritÃ¨res de SuccÃ¨s

### Training Sain
- âœ… Error train dÃ©croÃ®t progressivement
- âœ… Error val suit train (pas d'overfitting excessif)
- âœ… Pas de plateau aprÃ¨s epoch 10
- âœ… Error < 100% aprÃ¨s 50 epochs
- âœ… Error < 50% aprÃ¨s 100 epochs (idÃ©al)

### Mode Collapse Ã‰vitÃ©
- âœ… Error train â‰  Error val (variation normale)
- âœ… Variance prÃ©dictions > 0 (pas toujours mÃªme circuit)
- âœ… Distribution edge_types diversifiÃ©e (pas 90% R)
- âœ… Gradients varient (pas clippÃ©s 100% du temps)

### Performance Cible
- ğŸ¯ Combined error < 50%: Bon
- ğŸ¯ Combined error < 30%: TrÃ¨s bon
- ğŸ¯ Combined error < 20%: Excellent

## Questions?

Pour toute question ou problÃ¨me, vÃ©rifier:
1. Ce document (causes + solutions)
2. `GRAPH_REPRESENTATION.md` (format donnÃ©es)
3. `DEPLOYMENT_OVH.md` (dÃ©ploiement)
